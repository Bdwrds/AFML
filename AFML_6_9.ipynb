{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFML_6-9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3a0jisjF+c3TbKHqv6kYN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DoraTheExploring/Mitigating-Overfitting-Experiment/blob/master/AFML_6_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7Q2574O3jSi"
      },
      "source": [
        "# Advances in Financial Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB7vr0rb37fd"
      },
      "source": [
        "Implementations of the code snippets from the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL1h88x53o9Z"
      },
      "source": [
        "## Chapter 6: Ensemble Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhUHyhlO36gQ",
        "outputId": "9c06a627-8297-4442-cf96-d0a105454c85"
      },
      "source": [
        "## Snippet 6.1: Accuracy of the bagging classifier\n",
        "## from scipy.misc import comb #change in scipy\n",
        "\n",
        "from scipy.special import comb\n",
        "N,p,k=100,1./3,3.\n",
        "p_ = 0\n",
        "for i in range(0, int(N/k)+1):\n",
        "  p_ += comb(N,i)*p**i*(1-p)**(N-i)\n",
        "print(p, 1-p_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3333333333333333 0.4811966952738904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAGHiFUDv6xn"
      },
      "source": [
        "## Snippet 6.2: Three ways of setting up an RF\n",
        "# three alt methods for setting up an RF using different classes.\n",
        "# balanced_subsample will help prevent trees misclassifying minority trees.\n",
        "clf0 = RandomForestClassifier(n_estimators=1000, class_weight='balanced_subsample', criterion='entropy')\n",
        "\n",
        "clf1 = DecisionTreeClassifier(criterion='entropy', max_features='auto', class_weight='balanced')\n",
        "clf1 = BaggingClassifier(base_estimator=clf1, n_estimators=1000, max_samples=avgU)\n",
        "\n",
        "clf2 = RandomForestClassifier(n_estimators=1, criterion='entropy', bootstrap=False, class_weight='balanced_subsample')\n",
        "clf2 = BaggingClassifier(base_estimator=clf2, n_estimators=1000, max_samples=avgU, max_featurs=1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiGi8FIhv60c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgBMjVEI3pDs"
      },
      "source": [
        "## Chapter 7: Cross-Validation in Finance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL6K5SQp36Ee"
      },
      "source": [
        "\n",
        "## snippet 7.1: Purging Observations in the training set.\n",
        "\"\"\"\n",
        "Purges observations from training set.\n",
        "testTimes can be single series with single item spanning all testing set\n",
        "\"\"\"\n",
        "def getTrainTimes(t1, testTimes):\n",
        "  \"\"\"\n",
        "  Given testTimes, find the times of the training obs\n",
        "  -t1.index: Time when obs started.\n",
        "  -t1.value: Time when obs ended.\n",
        "  -testTimes: Times of testing observations\n",
        "  \"\"\"\n",
        "  trn = t1.copy(deep=True)\n",
        "  for i, i in testTimes.iteritems():\n",
        "    df0 = trn[(i<=trn.index)&(trn.index<=j)].index\n",
        "    df1 = trn[(i<=trn)&(trn<=j)].index\n",
        "    df2 = trn[(trn.index<=i)&(j<=trn)].index\n",
        "    trn = trn.drop(df0.union(df1).union(df2))\n",
        "  return trn\n",
        "\n",
        "\n",
        "## snippet 7.2:  Embargo on training obs\n",
        "\"\"\"\n",
        "Have an embargo of train observations after each test set.\n",
        "\"\"\"\n",
        "\n",
        "def getEmbargoTimes(times, pctEmbargo):\n",
        "  # Get embargo time for each bar\n",
        "  step = int(times.shape[0]*pctEmbargo)\n",
        "  if step==0:\n",
        "    mbrg = pd.Series(times, index=times)\n",
        "  else:\n",
        "    mbrg = pd.Series(times[step:], index=times[:-step])\n",
        "    mbrg = mbrg.append(pd.Series(times[-1], index=times[-step]))\n",
        "  return mbrg\n",
        "\n",
        "testTimes = pd.Series(mbrg[dt1], index=[dt0]) # include embargo before purge.\n",
        "trainTimes = getTrainTimes(t1, testTimes)\n",
        "testTimes = t1.loc(dt0:dt1).index\n",
        "\n",
        "\n",
        "## snippet 7.3: Cross-Val class when obs overlap\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "class PurgedKFold(_BaseKFold):\n",
        "  \"\"\"\n",
        "  Extend KFold ro work with labels that span intervals\n",
        "  The train is purged of obs overlapping test-label intervals\n",
        "  Test set is assumed contiguous (shuffle=False), w/o train examples in between\n",
        "  \"\"\"\n",
        "  def __init__(self, n_splits=3, t1=None, pctEmbargo=0.):\n",
        "    if not isinstance(t1, pd.Series):\n",
        "      raise ValueError('Label through dates mst be pandas series') \n",
        "    super(PurgedKFold, self).__init__(n_splits, shuffle=False, random_state=None)\n",
        "    self.t1=t1\n",
        "    self.pctEmbargo=pctEmbargo\n",
        "  \n",
        "  def split(self, X, y=None, groups=None):\n",
        "    if (X.index==self.t1.index).sum()!=len(self.t1):\n",
        "      raise ValueError('X and ThruDateValues must have same index')\n",
        "    indices = np.arange(X.shape[0])\n",
        "    mbrg=int(x.shape[0]*self.pctEmbargo)\n",
        "    test_starts=[(i[0], i[-1]+1)] for i in \\\n",
        "    np.array_split(np.arange(X.shape[0]), self.n_splits)]\n",
        "\n",
        "    for i, j in test_starts:\n",
        "      t0=self.t1.index[i]\n",
        "      test_indices=indices[i:j]\n",
        "      maxT1Idx=self.t1.index.searchsorted(self.t1[test_indices].max())\n",
        "      train_indices=self.t1.index.searchsorted(self.t1[self.t1<=t0].index)\n",
        "      train_indices=np.concatenate((train_indices, indices[maxT1Idx+mbrg:]))\n",
        "      yield train_indices, test_indices\n",
        "\n",
        "## snippet 7.4: Using the purged Class\n",
        "\n",
        "def cvScore(clf, X, y, sample_weight, scoring='neg_log_loss', t1=None, cv=None, cvGen=None, pctEmbargo=None):\n",
        "  if scoring not in ('neg_los_loss', 'accuracy'):\n",
        "    raise Exception('wrong scoring method')\n",
        "  from sklearn.metrics import log_loss, accuracy_score\n",
        "  from clfSequential import PurgedKFold\n",
        "  if cvGen is None:\n",
        "    cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)\n",
        "  score=[]\n",
        "  for train, test in cvGen.split(X=X):\n",
        "    fit = clf.fit(X=X.iloc[train,:]), y=y.iloc[train], sample_weight=sample_weight.iloc[train].values)\n",
        "    if scoring=='neg_los_loss':\n",
        "      prob= fit.predict_prob(X.iloc[test, :])\n",
        "      score_ = -log_loss(y.iloc[test], prob, sample_weight=sample_weight.iloc[test].values, labels=clf.classes_)\n",
        "    else:\n",
        "      pred=fit.predict(X.iloc[test, :])\n",
        "      score_= accuracy_score(y.iloc[test], pred, sample_weight=sample_weight.iloc[test].values)\n",
        "    score.append(scores_)\n",
        "  return np.array(score)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVVDovTy3pJJ"
      },
      "source": [
        "## Chapter 8: Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgHEeQek4E_h"
      },
      "source": [
        "Backtesting alone is not a valid research tool, but feature importance is.\n",
        "Many journals are filled with 'successful' strategies from a researchers optimized iteration of a backtest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRmmoZ3K3HNk"
      },
      "source": [
        "Sklearn examples here:\n",
        "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l375QNX135kH"
      },
      "source": [
        "#8.3.1 Mean Decrease Impurity\n",
        "# Snipper 8.2\n",
        "# set max_features to be 1 to give every feature opportunity.\n",
        "# same as default sklearn RF classifier implementation.\n",
        "\n",
        "def featImpMDI(fit, featNames):\n",
        "  #feat importance based on IS mean impurity reduction\n",
        "  df0 = {i:tree.feature_importances_ for i, tree in enumerate(fit.estimators)}\n",
        "  df0 = pd.DataFrame.from_dict(df0, orient='index')\n",
        "  df0.columns = featNames\n",
        "  df0 = df0.replace(0, np.nan) #because max_features=1\n",
        "  imp = pd.concat({'mean': df0.mean(), 'std':df0.std()*df0.shape[0]**-.5}, axis=1)\n",
        "  imp /= imp['mean'].sum()\n",
        "  return imp\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0-CEt_j-QGm"
      },
      "source": [
        "# 8.3.2 Mean Decrease Accuracy\n",
        "# Snippet 8.3: MDA feature importance\n",
        "# A slow OOS method.\n",
        "# Not limited to tree based methods, or just accuracy\n",
        "# As above, susceptible to substitution effects of correlated features.\n",
        "# Will consider one feature of identical two to be redundant.\n",
        "# Note, can product negative contributions to predictive power.\n",
        "\n",
        "def featImpMDA(clf, X, y, cv, sample_weight, t1, pctEmbargo, scoring='neg_log_loss'):\n",
        "  # feat importanced based on OOS score reduction\n",
        "  if scoring not in ['neg_los_loss', 'accuracy']:\n",
        "    raise Exception('wrong scoring method')\n",
        "  from sklearn.metrics import log_loss, accuracy_score\n",
        "  cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo) #purged cv\n",
        "  scr0, scr1 = pd.Series(), pd.DataFrame(columns=X.columns)\n",
        "  for i, (train, test) in enumerate(cvGen.split(X=X)):\n",
        "    X0, y0, w0 = X.iloc[train,:], y.iloc[train], sample_weight.iloc[train]\n",
        "    X1, y1, w1 = X.iloc[test,:], y.iloc[test], sample_weight.iloc[test]\n",
        "    fit = clf.fit(X=X0, y=y0, sample_weight=w0.values)\n",
        "    if scoring=='neg_log_loss':\n",
        "      prob=fit.predict_prob(X1)\n",
        "      scr0.loc[i]=-log_loss(y1, prob, sample_weight=w1.values, labels=clf.classes_)\n",
        "    else:\n",
        "      pred=fit.predict(X1)\n",
        "      scr0.loc[i]=accuracy_score(y1, pred, sample_weight=w1.values)\n",
        "    for j in X.columns:\n",
        "      X1_=X1.copy(deep=True)\n",
        "      np.random.shuffle(X1_[j].values) #permutation of single col\n",
        "      if scoring=='neg_log_loss':\n",
        "        prob=fit.predict_prob(X1_)\n",
        "        scr1.loc[i,j]=-log_loss(y1, prob, sample_weight=w1.values, labels=clf.classes_)\n",
        "      else:\n",
        "        pred=fit.predict(X1_)\n",
        "        scr1.loc[i,j]=-log_loss(y1, prob, sample_weight=w1.values)\n",
        "  imp=(-scr1).add(scr0, axis=0)\n",
        "  if scoring=='neg_log_loss': imp=imp/-scr1\n",
        "  else: imp=imp/(1.-scr1)\n",
        "  imp=pd.concat({'mean':imp.mean(), 'std':imp.std()*imp.shape[0]**-.5}, axis=1)\n",
        "  return imp, scor0.mean()\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz1OeaJSE_U-"
      },
      "source": [
        "# 8.4 FI without substitution effects\n",
        "\"\"\"\n",
        "Sub effects can lead us to discard feaures that are redundant.\n",
        "This can be a problem when trying to understand a model\n",
        "\"\"\"\n",
        "# Snippet 8.4\n",
        "# Single Feature Importance\n",
        "# OOS performance of each feature in isolation\n",
        "# Works for any classifier, and other metrics beyond accuracy.\n",
        "\n",
        "def auxFeatImpSFI(featNames, clf, trnsX, cont, scoring, cvGen):\n",
        "  imp = pd.DataFrame(columns=['mean', 'std'])\n",
        "  for featName in featNames:\n",
        "    df0=cvScore(clf, x=trnsX[[featName]], y=cont['bin'], sample_weight=cont['w'], scoring=scoring, cvGen=cvGen)\n",
        "    imp.loc[featName, 'mean'] = df0.mean()\n",
        "    imp.loc[featName, 'std'] = df0.std()*df0.shape[0]**-.5\n",
        "  return imp\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm24lWHWE_Xj"
      },
      "source": [
        "# Snippet 8.5 Comp Orthogonal Features\n",
        "\"\"\"\n",
        "Sub effects dilute the imp of features by MDI, and underestimate the effects from MDA.\n",
        "A partial solution is via PCA, alleviating the impact of linear sub effects.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_eVec(dot, varThres):\n",
        "  # compute eVec fom dot prod matrix, reduce dimension\n",
        "  eVal, eVec = np.linalg.eigh(dot)\n",
        "  idx = eVal.argsort()[::-1]\n",
        "  eVal, evec = eVal[idx], eVec[:,idx]\n",
        "  #2) only pos eVals\n",
        "  eVal = pd.Series(eVal, index=['PC_'+str(i+1) for i in range(eVal.shape[0])])\n",
        "  eVec = pd.DataFrame(eVec, index=dot.index, columns=eVal.index)\n",
        "  eVec = eVec.loc[:, eVal.index]\n",
        "  #3) reduce dimension, form PCs\n",
        "  cumVar = eVal.cumsum()/eVal.sum()\n",
        "  dim = cumVar.values.searchsorted(varThres)\n",
        "  eVal, eVec = eVal.iloc[:dim+1], eVec.iloc[:,:dim+1]\n",
        "  return eVal, eVec\n",
        "\n",
        "def orthoFeats(dfX, varThres=.95):\n",
        "  # Given dataframe dfX of features, compute orthfeat dfP\n",
        "  dfZ = dfX.sub(dfX.mean(), axis=1).div(dfX.std(), axis=1) #standardize\n",
        "  dot = pd.DataFrame(np.dot(dfZ.T, dfZ), index=dfX.columns, columns=dfX.columns)\n",
        "  eVal, eVec = get_eVec(dot, varThres)\n",
        "  dfP = np.dot(dfZ, eVec)\n",
        "  return dfP\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thb2zQn1E_aK",
        "outputId": "64854371-1804-4548-a7bd-62536e1d250f"
      },
      "source": [
        "## Snippet 8.6 Computation of weighted kendalls taus between feature imp and invest PCA ranking.\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import weightedtau\n",
        "featImp = np.array([.55, .33, .07, .05]) # feat imp\n",
        "pcaRank = np.array([1, 2, 4, 3]) # pca rank\n",
        "weightedtau(featImp, pcaRank**-1.)[0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8133333333333331"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPpwsUD9E_dA"
      },
      "source": [
        "## Synthetic Data\n",
        "\"\"\"\n",
        "Test how feat imp methods respond to synthetic datasets.\n",
        "Generate a dataset composed on three kinds of features:\n",
        "- Informative: Features used to determine the label\n",
        "- Redundant: Random linear combinations of informative features - cause substitution effetcts.\n",
        "- Noise: Features that have no bearing on determings the observations label.\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
        "Generate a random n-class classification problem.\n",
        "# Changes\n",
        "- Datetimeindex to date_range\n",
        "- xrange to range \n",
        "\"\"\"\n",
        "## Snippet 8.7: Creating a synthetic dataset\n",
        "import pandas as pd\n",
        "\n",
        "def getTestData(n_features=40, n_informative=10, n_redundant=10, n_samples=10000):\n",
        "  # generate random dataset for classification problem\n",
        "  from sklearn.datasets import make_classification\n",
        "  trnsX, cont = make_classification(n_samples=n_samples, \n",
        "                                    n_features=n_features,\n",
        "                                    n_informative=n_informative, n_redundant=n_redundant,\n",
        "                                    random_state=0, shuffle=True)\n",
        "  #df0 = pd.DatetimeIndex(periods=n_samples, freq=pd.tseries.offsets.BDay(), \\\n",
        "  df0 = pd.date_range(periods=n_samples, freq=pd.tseries.offsets.BDay(), \\\n",
        "                         end=pd.datetime.today())\n",
        "  trnsX, cont = pd.DataFrame(trnsX, index=df0), pd.Series(cont, index=df0).to_frame('bin')\n",
        "  df0 = ['I_'+str(i) for i in range(n_informative)]+['R_'+str(i) for i in range(n_redundant)]\n",
        "  df0 += ['N_'+str(i) for i in range(n_features-len(df0))]\n",
        "  trnsX.columns = df0\n",
        "  cont['w'] = 1./cont.shape[0]\n",
        "  cont['t1'] = pd.Series(cont.index, index=cont.index)\n",
        "  return trnsX, cont\n",
        "\n",
        "\n",
        "t, c = getTestData()\n",
        "print(t)\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAhzrEvwv9xE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdSec1SvE_gL"
      },
      "source": [
        "## Snippet 8: Calling feat imp for any method\n",
        "\n",
        "def featImportance(trnsX, cont, n_estimators=1000, cv=10, max_samples=1., \\\n",
        "                   numThreads=24, pctEmbargo=0, scoring='accuracy', method = 'SFI', \\\n",
        "                   minWLeaf=0., **kargs):\n",
        "  # feat imp from random forest\n",
        "  from sklearn.tree import DecisionTreeClassifier\n",
        "  from sklearn.ensemble import BaggingClassifier\n",
        "  from mpEngine import mpPandasObj\n",
        "  n_jobs = (-1 if numThreads>1 else 1)\n",
        "  # 1) prepare classifier, cv. max_features=1, to prevent masking.\n",
        "  clf = DecisionTreeClassifier(criteron='entropy', max_features=1,\n",
        "                               class_weight='balanced', min_weight_fraction=minWLeaf)\n",
        "  clf = BaggingClassifier(base_estimator=clf, n_estimators=n_estimators, \\\n",
        "                          max_features=1., max_samples=max_samples, \\\n",
        "                          oob_score=True, n_jobs=n_jobs)\n",
        "  fit = clf.fit(X=trnsX, y= cont['bin'], sample_weight=cont['w'].values)\n",
        "  oob = fit.oob_score\n",
        "  if method=='MDI':\n",
        "    imp = featImpMDI(fit, featNames, trnsX.columns)\n",
        "    oos = cvScore(clf, X=trnsX, y = cont['bin'], cv=cv, sample_weight=cont['w'],\\\n",
        "                  t1=cont['t1'], pctEmbargo=pctEmbargo, scoring=scoring).mean()\n",
        "  elif method=='MDA':\n",
        "    imp, oos = featImpMDA(clf, X=trnsX, y = cont['bin'], sample_weight=cont['w'],\\\n",
        "                          t1=cont['t1'], pctEmbargo=pctEmbargo, scoring=scoring)\n",
        "  elif method =='SFI':\n",
        "    cvGen = PurgedKFold(n_splits=cv, t1=cont['t1'], pctEmbargo=pctEmbargo)\n",
        "    oos = cvScore(clf, X=trnsX, y = cont['bin'], sample_weight=cont['w'], \\\n",
        "                  scoring=scoring, cvGen = cvGen).mean()\n",
        "    clf.n_jobs=1 # para auxfeatImpSFI over clf\n",
        "    imp = mpPandasObj(auxfeatImpSFI, ('featNames', trnsX.columns), numThreads, \\\n",
        "                      clf = clf, trnsX=trnsX, cont=cont, scoring=scoring, \\\n",
        "                      cvGen=cvGen )\n",
        "  return imp, oob, oos\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLoZm7fcE_jK"
      },
      "source": [
        "## Snippet 8.9: Calling all functions\n",
        "\n",
        "def testFunc(n_features=40, n_informative=10, n_redundant=10, n_estimators=1000,\\\n",
        "             n_samples = 10000, cv=10):\n",
        "  # test the perf of feat imp functions of artificial data\n",
        "  # Nr noise features = n_features-n_informative-n_redundant\n",
        "  trnsX, cont=getTestData(n_features, n_informative, n_redundant, n_samples)\n",
        "  dict0  = {'minWLead':[0.], 'scoring':['accuracy'], 'method':['MDI', 'MDA','SFI'],\\\n",
        "            'max_samples':[1.]}\n",
        "  jobs, out = (dict(izip(dict0, i)) for i in product(*dict0.values())), []\n",
        "  kargs = {'pathOut':'./testFunc', 'n_estimators': n_estimators, \\\n",
        "           'tag':'testFunc','cv':cv}\n",
        "  for job in jobs:\n",
        "    job['simNum']=job['method']+'_'+job['scoring']+'_'+'%.2f'%job['minWLeaf']+\\\n",
        "    '_'+str(job['max_samples'])\n",
        "    print(job['simNum'])\n",
        "    kargs.update(job)\n",
        "    imp, oob, oos = featImportance(trnsX=trnsX, cont=cont, **kargs)\n",
        "    plotFeatImportance(imp=imp, oob=oob, oos=oos, **kargs)\n",
        "    df0 = imp[['mean']]/imp['mean'].abs().sum()\n",
        "    df0['type'] = [i[0] for i in df0.index]\n",
        "    df0 = df0.groupby('type')['mean'].sum().to_dict()\n",
        "    df0.update({'oob':oob, 'oos':oos}); df0.update(job)\n",
        "    out.append(df0)\n",
        "  out = pd.DataFrame(out).sort_values(['method', 'scoring', 'minWLeaf', 'max_samples'])\n",
        "  out = out['method', 'scoring', 'minWLeaf', 'max_samples', 'I', 'R', 'N', 'oob', 'oos']\n",
        "  out.to_csv(kargs['pathOut']+'stats.csv')\n",
        "  return\n",
        "\n",
        "\n",
        "               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpr5SWWGE_lK"
      },
      "source": [
        "## Snippet 8.10\n",
        "\n",
        "def plotFeatImportance(pathOut, imp, oob, oos, method, tag=0, simNum=0, **kargs):\n",
        "  # plot mean imp bars with stf\n",
        "  mp1.figure(figsize=(10, imp.shape[0]/5.))\n",
        "  imp = imp.sort_values('mean', ascending=True)\n",
        "  ax = imp['mean'].plot(kind='barh', color='b', alpha=.25, xerr=imp['std'], \\\n",
        "                        error_kw={'ecolor':'r'})\n",
        "  if method=='MDI':\n",
        "    mp1.xlim([0, imp.sum(axis=1).max()])\n",
        "    mp1.axvline(1./imp.shape[0], linewidth=1, coor='r', linestyle='dotted')\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "  for i, j in zip(ax.patches, imp.index):ax.text(i.get_width()/2,\\\n",
        "                                                 i.get_y()+i.get_height()/2, \\\n",
        "                                                 j, ha='center', va='center',\n",
        "                                                 color='black')\n",
        "  mp1.title('tag='+tag+' | sumNum='+str(simNum)+' | oob='+str(round(oob, 4))+\\\n",
        "            ' | oos='+str(round(oos, 4)))\n",
        "  mp1.savefig(pathOut+'featImportance_'+str(simNum)+'.png', dpi=100)\n",
        "  mp1.clf(); mp1.close()\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8p05Dxl3pSE"
      },
      "source": [
        "## Chapter 9: Hyper-Parameter Tuning with Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ey-H0OkkepX"
      },
      "source": [
        "Chapter focus on tuning hyper-parameters using the CV approach to try and minimise overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgNdFMXPpFw3"
      },
      "source": [
        "### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WsFBNsQ3cYC"
      },
      "source": [
        "## Snippet 9.1: Grid search with purged k-fold cv\n",
        "\n",
        "def clfHyperFit(feat, lbl, t1, pipe_clf, param_grid, cv=3, bagging=[0, None, 1.],\\\n",
        "                n_jobs=-1, pctEmbargo=0, **fit_params):\n",
        "  if set(lbl.values)=={0,1}: scoring='f1' #f1 for meta labelling\n",
        "  else: scoring='neg_log_loss' # symmetric towards all cases\n",
        "  # 1) hyperparamter search, on train data\n",
        "  inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo) #purged\n",
        "  gs = GridSearchCV(estimator=pipe_clf, param_grid=param_grid,\n",
        "                    scoring=scoring, cv=inner_cv, n_jobs = n_jobs, iid=False)\n",
        "  gs = gs.fit(feat, lbl, **fit_params).best_estimator_ # pipeline\n",
        "  # 2) fit validated model on entirety of data\n",
        "  if bagging[1]>0:\n",
        "    gs = BaggingClassifier(base_estimator=MyPipeline(gs.steps),\n",
        "                           n_estimators=int(bagging[0]), max_samples=float(bagging[1]),\n",
        "                                            max_features=float(bagging[2]), n_jobs=n_jobs)\n",
        "    gs = gs.fit(feat, lbl, sample_weight=fit_param \\\n",
        "                [gs.base_estimator.steps[-1][0]+'_sample_weight'])\n",
        "    gs = Pipeline([('bag',gs)])\n",
        "  return gs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktlvDrxJ3i4P"
      },
      "source": [
        "## Snippet 9.2: Enhanced pipeline class\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class MyPipeline(Pipeline):\n",
        "  def fit(self, X, y, sample_weight=None, **fit_params):\n",
        "    if sample_weight is not None:\n",
        "      fit_params[self.steps[-1][0]+'_sample_weight']= sample_weight\n",
        "    return super(MyPipeline, self).fit(X, y, **fit_params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-xCbc3cpIEz"
      },
      "source": [
        "### Randomized search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSBdNZw-3i7B"
      },
      "source": [
        "## Snipet 9.3: Randomized search with purged k-fold cv\n",
        "# variation on the above to be less computationally onerous\n",
        "def clfHyperFit(feat, lbl, pipe_clf, param_grid, cv=3, bagging=[0, Nonem 1.],\n",
        "                rndSearchIter=0, n_jobs=-1, pctEmbargo=0, **fit_params):\n",
        "  if set(lbl.values)=={0,1}: scoring='f1' # f1 for meta-labelling\n",
        "  else: scoring='neg_los_loss' # symmetric towards all cases\n",
        "  # 1) hyperparameter search on train set\n",
        "  inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo) # embargo\n",
        "  if rndSearchIter==0:\n",
        "    gs= GridSearchCV(estimator=pipe_clf, param_grid=param_grid, \n",
        "                     scoring=scoring, cv=inner_cv, n_jobs=n_jobs, iid=False)\n",
        "  else:\n",
        "    gs = RandomizedSearchCV(estimator=pipe_clf, param_distribution= \\\n",
        "                            param_grid, scoring=scoring, cv=inner_cv, \\\n",
        "                            n_jobs=n_jobs, iid=False, n_iter=rndSearchIter)\n",
        "  gs.fit(feat, lbl, **fit_params).best_estimator_ # pipeline\n",
        "  #2) fit validated model on entirety of data\n",
        "  if bagging[1]>0:\n",
        "    gs = BaggingClassifier(base_estimator=MyPipeline(gs.steps), \n",
        "                           n_estimators=int(bagging[0]), max_samples=float(bagging[1]),\n",
        "                           max_featurs=float(bagging[2]), n_jobs=n_jobs)\n",
        "    gs = gs.fit(feat, lbl, sample_weight=fit_params \\\n",
        "                [gs.base_estimator.steps[-1][0]+'_sample_weight'])\n",
        "    gs = Pipeline([('bag', gs)])\n",
        "  return gs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "1RzJyURW3i9e",
        "outputId": "5efd658d-cfa2-42bd-8744-7b7db714dd2b"
      },
      "source": [
        "## Snippet 9.4: The logUniform_gen Class\n",
        "\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as mpl\n",
        "from scipy.stats import rv_continuous, kstest\n",
        "\n",
        "class logUniform_gen(rv_continuous):\n",
        "  # random num log-uniform distribted between 1 and e.\n",
        "  def _cdf(self, x):\n",
        "    return np.log(x/self.a)/np.log(self.b/self.a)\n",
        "\n",
        "def logUniform(a=1, b=np.exp(1)): return logUniform_gen(a=a, b=b, name='logUniform')\n",
        "\n",
        "a,b, size= 1E-3, 1E3, 10000\n",
        "vals = logUniform(a=a, b=b).rvs(size=size)\n",
        "\n",
        "print(kstest(rvs = np.log(vals), cdf= 'uniform', args=(np.log(a), np.log(b/a)), N=size))\n",
        "print(pd.Series(vals).describe())\n",
        "\n",
        "mpl.subplot(1,2,1)\n",
        "pd.Series(np.log(vals)).hist()\n",
        "mpl.subplot(122)\n",
        "pd.Series(vals).hist()\n",
        "mpl.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KstestResult(statistic=0.015085428978270171, pvalue=0.021104745815026155)\n",
            "count    10000.000000\n",
            "mean        72.890029\n",
            "std        175.710904\n",
            "min          0.001003\n",
            "25%          0.037163\n",
            "50%          1.113897\n",
            "75%         33.791609\n",
            "max        993.876034\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbndCzas3i_V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pinQTW9s5V4C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQuobcGM4nJ4"
      },
      "source": [
        "from pandas_datareader.famafrench import get_available_datasets\n",
        "import pandas_datareader.data as web\n",
        "ds = web.DataReader('F-F_Research_Data_Factors_daily', 'famafrench', start='1990-08-30')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqfikOzm5FBv",
        "outputId": "6cf46f2b-6408-41f9-cc25-30b619487999"
      },
      "source": [
        "ds.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([0, 'DESCR'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEjY0SAd4ziQ",
        "outputId": "bb0bb901-99fc-4853-a9ff-5c4abb5cd369"
      },
      "source": [
        "print(ds['DESCR'])\n",
        "print(ds[0].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F-F Research Data Factors daily\n",
            "-------------------------------\n",
            "\n",
            "This file was created by CMPT_ME_BEME_RETS_DAILY using the 202103 CRSP database. The Tbill return is the simple daily rate that, over the number of trading days in the month, compounds to 1-month TBill rate from Ibbotson and Associates Inc. Copyright 2021 Kenneth R. French\n",
            "\n",
            "  0 : (7705 rows x 4 cols)\n",
            "            Mkt-RF   SMB   HML     RF\n",
            "Date                                 \n",
            "1990-08-30   -1.39  0.93  0.35  0.028\n",
            "1990-08-31    0.95 -0.77 -0.22  0.028\n",
            "1990-09-04    0.09 -0.28 -0.28  0.031\n",
            "1990-09-05    0.35  0.18 -0.15  0.031\n",
            "1990-09-06   -1.06  0.45  0.60  0.031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec7hQML65Wxh"
      },
      "source": [
        "data = ds[0]\n",
        "data = data.dropna()\n",
        "data = data/100 #convert to percent returns\n",
        "RF_data = (1+data['RF']).cumprod()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}